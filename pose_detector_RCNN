import os
import sys
import argparse
import logging
from pathlib import Path
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet18, ResNet18_Weights
from torchvision.ops import roi_align
from pycocotools.coco import COCO
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

# -------------------------
# apparently necessary 
# -------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--ann", help="COCO keypoint annotations JSON")
parser.add_argument("--images", help="COCO images directory")
parser.add_argument("--epochs", type=int, default=10)
parser.add_argument("--batch", type=int, default=4)
parser.add_argument("--lr", type=float, default=1e-4)
parser.add_argument("--input_size", type=int, default=512)
parser.add_argument("--out", type=str, default="./ckpt")
parser.add_argument("--batches_per_epoch", type=int, default=1000)
parser.add_argument("--run_demo", action="store_true")
parser.add_argument("--resume_ckpt", type=str)
args = parser.parse_args()

# -------------------------
# line drawer
# -------------------------
coco_skeleton = [
    (15, 13), (13, 11), (16, 14), (14, 12), (11, 12),
    (5, 11), (6, 12), (5, 6), (5, 7), (6, 8), 
    (7, 9), (8, 10), (1, 2), (0, 1), (0, 2), (1, 3), (2, 4)
]

# -------------------------
# import dataset (claude)
# -------------------------
class CocoKeypointDataset(Dataset):
    def __init__(self, ann_file: str, images_dir: str, input_size: int = 512):
        logger.info(f"Loading COCO from {ann_file}")
        self.coco = COCO(str(ann_file))
        self.images_dir = Path(images_dir)
        self.ids = sorted(self.coco.getImgIds())
        self.input_size = input_size
        self.failed_images = set() # Stores paths of failed images
        logger.info(f"Dataset: {len(self.ids)} images")

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        # We use a large number of attempts, cycling through the dataset once
        max_attempts = len(self.ids) 
        current_idx = idx
        
        for attempt in range(max_attempts):
            
            try:
                # 1. Get image info and path
                img_info = self.coco.loadImgs(self.ids[current_idx])[0]
                path = self.images_dir / img_info['file_name']
                path_str = str(path)
                
                # Check if we've seen this fail before
                if path_str in self.failed_images:
                    # Move to the next index and continue to the next attempt
                    current_idx = (current_idx + 1) % len(self.ids)
                    continue
                
                # 2. Load image
                img = cv2.imread(path_str)
                
                if img is None:
                    # Image file could not be read (root cause of user error)
                    logger.warning(f"Failed to load {path_str}. Check path/integrity.")
                    self.failed_images.add(path_str)
                    current_idx = (current_idx + 1) % len(self.ids)
                    continue
                
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                H, W = img.shape[:2]
                
                # Resize keeping aspect ratio
                scale = self.input_size / max(H, W)
                new_h, new_w = int(H * scale), int(W * scale)
                img = cv2.resize(img, (new_w, new_h))
                
                # 3. Get annotations and resize keypoints/boxes
                ann_ids = self.coco.getAnnIds(imgIds=img_info['id'], iscrowd=False)
                anns = self.coco.loadAnns(ann_ids)
                
                boxes, kps = [], []
                for ann in anns:
                    if 'keypoints' not in ann or ann.get('num_keypoints', 0) == 0:
                        continue
                    x, y, w, h = ann['bbox']
                    x1, y1 = x * scale, y * scale
                    x2, y2 = (x + w) * scale, (y + h) * scale
                    if x2 - x1 < 10 or y2 - y1 < 10:
                        continue
                    boxes.append([x1, y1, x2, y2])
                    
                    k = np.array(ann['keypoints'], dtype=np.float32).reshape(-1, 3)
                    k[:, :2] *= scale
                    kps.append(k[:17])
                    
                if len(boxes) == 0:
                    # Skip images with no valid annotations
                    current_idx = (current_idx + 1) % len(self.ids)
                    continue
                
                boxes = np.array(boxes, dtype=np.float32)
                kps = np.array(kps, dtype=np.float32)
                
                # 4. Prepare tensors and return
                img_t = torch.from_numpy(img.astype(np.float32) / 255.0).permute(2, 0, 1)
                return img_t, boxes, kps
                
            except Exception as e:
                # Catch unexpected errors during processing
                logger.error(f"Error processing index {current_idx} ({path_str}): {e}")
                self.failed_images.add(path_str)
                current_idx = (current_idx + 1) % len(self.ids)
                
            # If all images have been added to failed_images, break to return dummy data
            if len(self.failed_images) == len(self.ids):
                break
        
        # If the loop finishes without returning valid data, return dummy data
        logger.error(f"Failed to load any valid image starting from index {idx}.")
        dummy_img = torch.zeros((3, self.input_size, self.input_size))
        dummy_boxes = np.zeros((0, 4), dtype=np.float32)
        dummy_kps = np.zeros((0, 17, 3), dtype=np.float32)
        return dummy_img, dummy_boxes, dummy_kps

def collate_fn(batch):
    imgs, boxes_list, kps_list = zip(*batch)
    max_h = max(im.shape[1] for im in imgs)
    max_w = max(im.shape[2] for im in imgs)
    padded = [F.pad(im, (0, max_w - im.shape[2], 0, max_h - im.shape[1])) for im in imgs]
    return torch.stack(padded), list(boxes_list), list(kps_list)

# -------------------------
# generate the "label" for the CNN to hit. 
# -------------------------
def make_heatmap_targets(rois, gt_kps, out_size=64, sigma=2.0):
    # some np stuff i had to understand 
    N, K = rois.shape[0], gt_kps.shape[1]
    targets = np.zeros((N, K, out_size, out_size), dtype=np.float32)
    
    for i in range(N):
        x1, y1, x2, y2 = rois[i]
        w, h = max(x2 - x1, 1), max(y2 - y1, 1)
        
        for k in range(K):
            xk, yk, v = gt_kps[i, k]
            if v <= 0:
                continue
            
            nx, ny = (xk - x1) / w, (yk - y1) / h
            px, py = nx * (out_size - 1), ny * (out_size - 1)
            
            if 0 <= px < out_size and 0 <= py < out_size:
                y_grid, x_grid = np.ogrid[:out_size, :out_size]
                # take gaussian distribution
                gaussian = np.exp(-((x_grid - px)**2 + (y_grid - py)**2)/(2 * sigma**2))
                targets[i, k] = np.maximum(targets[i, k], gaussian)
    
    return targets

# -------------------------
# CNN
# -------------------------
class KeypointRCNN(nn.Module):
    def __init__(self, num_kp=17, pretrained=True):
        super().__init__()
        resnet = resnet18(weights=ResNet18_Weights.DEFAULT if pretrained else None)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])
        self.out_channels = 512
        self.stride = 32
        # very long 
        self.kp_head = nn.Sequential(
            nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, num_kp, 1)
        )
        self.heatmap_size = 64
        
    def forward(self, images, proposals):
        feat = self.backbone(images)
        if proposals.shape[0] == 0:
            return torch.zeros((0, 17, self.heatmap_size, self.heatmap_size), device=images.device)
        
        pooled = roi_align(feat, proposals, output_size=(14, 14), spatial_scale=1.0/self.stride, sampling_ratio=2)
        heatmaps = self.kp_head(pooled)
        return heatmaps

# -------------------------
# training
# -------------------------
def train(model, dataloader, optimizer, device, batches_per_epoch):
    model.train()
    total_loss = 0.0
    pbar = tqdm(dataloader, total=min(len(dataloader), batches_per_epoch))
    
    for batch_idx, (imgs, boxes_list, kps_list) in enumerate(pbar):
        if batch_idx >= batches_per_epoch:
            break
        
        imgs = imgs.to(device)
        
        # build proposals
        proposals = []
        gt_kps_list = []
        for b_idx, (boxes, kps) in enumerate(zip(boxes_list, kps_list)):
            if boxes.shape[0] > 0:
                for box in boxes:
                    proposals.append([b_idx, box[0], box[1], box[2], box[3]])
                gt_kps_list.append(kps)
        
        if len(proposals) == 0:
            continue
        
        proposals = torch.tensor(proposals, dtype=torch.float32, device=device)
        gt_kps = np.vstack(gt_kps_list)
        
        # generate targets for the algorithm
        rois = proposals[:, 1:].cpu().numpy()
        targets = make_heatmap_targets(rois, gt_kps, out_size=model.heatmap_size, sigma=2.0)
        targets = torch.from_numpy(targets).to(device)
        
        # zero grads
        optimizer.zero_grad()
        preds = model(imgs, proposals)
        
        # prevent errors so the code doesn't break if 1 unclean data is and instead skips
        if preds.shape != targets.shape:
             continue
        
        # loss 
        # only calculate loss where there are actual keypoints (non-zero targets)
        # this prevents the model from gaming the loss by predicting zeros everywhere
        positive_mask = (targets > 0.01).float()  # Mask for actual keypoint locations
        num_positive = positive_mask.sum()
        
        if num_positive > 0:
            loss = (F.mse_loss(preds, targets, reduction='none') * positive_mask).sum() / num_positive
        else:
            continue
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': total_loss / (batch_idx + 1)})
    
    return total_loss / max(1, min(len(dataloader), batches_per_epoch))

# -------------------------
# draw "skeleton" -- this is what coco calls it 
# -------------------------
def draw_skeleton(img, kps, threshold=0.3):
    vis = img.copy()
    H, W = img.shape[:2]
        
    # line drawing logic
    for a, b in coco_skeleton:
        if a >= len(kps) or b >= len(kps):
            continue
        xa, ya, sa = kps[a]
        xb, yb, sb = kps[b]
        
        if sa >= threshold and sb >= threshold:
            #clip the lines so they aren't too long 
            pt1 = (int(np.clip(xa, 0, W-1)), int(np.clip(ya, 0, H-1)))
            pt2 = (int(np.clip(xb, 0, W-1)), int(np.clip(yb, 0, H-1)))
            cv2.line(vis, pt1, pt2, (0, 255, 0), 2, cv2.LINE_AA)

    # dot drawing logic
    for i, (x, y, s) in enumerate(kps):
        if s >= threshold:
            #diag
            print(f'successfuly passed threshold: kp {i}, x={x:.1f}, y={y:.1f}, s={s:.3f}')
            if np.isfinite(x) and np.isfinite(y):
                pt = (int(np.clip(x, 0, W-1)), int(np.clip(y, 0, H-1)))
                cv2.circle(vis, pt, 4, (0, 255, 0), -1)
                cv2.circle(vis, pt, 5, (255, 255, 255), 1)
            else:
                # ignore bugs so it doesn't crash
                print(f'  -> Skipped: non-finite coordinates')
    
    return vis

#gemini wrote this next bit but now i understand it
def predict_frame(model, frame_bgr, input_size=512, device='cpu', threshold=0.3):
    #just scaling to make ui look good 
    H, W = frame_bgr.shape[:2]
    scale = input_size / max(H, W)
    new_h, new_w = int(H * scale), int(W * scale)
    
    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    img = cv2.resize(frame_rgb, (new_w, new_h))
    
    #padding? 
    padded_img = np.zeros((input_size, input_size, 3), dtype=np.float32)
    padded_img[:new_h, :new_w] = img
    
    img_t = torch.from_numpy(padded_img / 255.0).permute(2, 0, 1).unsqueeze(0).to(device)
    
    #send in the edited image
    proposals = torch.tensor([[0, 0, 0, new_w, new_h]], dtype=torch.float32, device=device)
    
    with torch.no_grad():
        heatmaps = model(img_t, proposals).squeeze(0).cpu().numpy()
    
    kps = []
    heatmap_res = heatmaps.shape[1]  # Actual output size, not model.heatmap_size 
    
    print(f"Heatmap shape: {heatmaps.shape}, max values: {heatmaps.max(axis=(1,2))}")
    
    for k in range(16+1):
        hm = heatmaps[k]
        hm_smooth = cv2.GaussianBlur(hm, (3, 3), 0)
        
        flat_idx = hm_smooth.argmax()
        y_idx, x_idx = divmod(flat_idx, heatmap_res)
        
        score = float(hm_smooth[y_idx, x_idx])
        
        if score > 0.01:
            y_window = slice(max(0, y_idx-2), min(heatmap_res, y_idx+3))
            x_window = slice(max(0, x_idx-2), min(heatmap_res, x_idx+3))
            local_region = hm_smooth[y_window, x_window]
            
            if local_region.size > 0:
                y_coords, x_coords = np.mgrid[y_window, x_window]
                total_mass = local_region.sum()
                if total_mass > 0:
                    y_idx = float((y_coords * local_region).sum() / total_mass)
                    x_idx = float((x_coords * local_region).sum() / total_mass)
        
       # reverse our keypoint encoder
        x_img = (x_idx / (heatmap_res - 1)) * new_w
        y_img = (y_idx / (heatmap_res - 1)) * new_h
        
        x_orig = x_img / scale
        y_orig = y_img / scale
        kps.append((x_orig, y_orig, score))
    
    print(f"Keypoint scores: {[f'{s:.3f}' for x,y,s in kps]}")
    print(f"Threshold: {threshold}, Passing: {sum(1 for x,y,s in kps if s >= threshold)}")

    vis = draw_skeleton(frame_bgr, kps, threshold)
    return vis, kps

# basic webcam
def run_webcam_demo(model, input_size=512, device='cpu'):
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Webcam does not work.")
        return
    
    threshold = 0.05
    print("Press 'q' to quit, 'o' to decrease threshold (-0.01), 'p' to increase threshold (+0.01)")
    print(f"Current threshold: {threshold:.2f}")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        vis, _ = predict_frame(model, frame, input_size, device, threshold=threshold) 
        
        # display threshold (looks nice)
        cv2.putText(vis, f"Threshold: {threshold:.2f}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        cv2.imshow("Keypoint Demo", vis) 
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('o'):
            threshold = max(0.0, threshold - 0.01)
            print(f"Threshold decreased to: {threshold:.2f}")
        elif key == ord('p'):
            threshold = min(1.0, threshold + 0.01)
            print(f"Threshold increased to: {threshold:.2f}")
    
    cap.release()
    cv2.destroyAllWindows()

# -------------------------
# main loop 
# -------------------------
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # demo mode
    if args.run_demo:
        if not args.resume_ckpt:
            print("Provide --resume_ckpt for demo mode")
            sys.exit(1)
        
        model = KeypointRCNN(pretrained=False).to(device)
        ckpt = torch.load(args.resume_ckpt, map_location=device)
        model.load_state_dict(ckpt['model'])
        model.eval()
        run_webcam_demo(model, args.input_size, device)
        sys.exit(0)
    
    # training mode
    if not args.ann or not args.images:
        print("Provide --ann and --images for training")
        # yay for ui
        sys.exit(1)
    
    os.makedirs(args.out, exist_ok=True)
    
    print(f"Creating dataset...")
    dataset = CocoKeypointDataset(args.ann, args.images, args.input_size)
    dataloader = DataLoader(dataset, batch_size=args.batch, shuffle=True, 
                           collate_fn=collate_fn, num_workers=0)
    
    print(f"Initializing model...")
    model = KeypointRCNN(pretrained=True).to(device)
    # if you need to change lr do it here
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)
    
    logger.info(f"Training on {device} for {args.epochs} epochs")
    
    for epoch in range(args.epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{args.epochs}")
        print(f"{'='*60}")
        loss = train(model, dataloader, optimizer, device, args.batches_per_epoch)
        scheduler.step()
        logger.info(f"Epoch {epoch+1}/{args.epochs} - Loss: {loss:.4f}")
        
        ckpt_path = os.path.join(args.out, f"epoch_{epoch+1}.pth")
        torch.save({'model': model.state_dict(), 'epoch': epoch+1}, ckpt_path)
        print(f"Saved: {ckpt_path}")
    
    final_path = os.path.join(args.out, "model_final.pth")
    torch.save({'model': model.state_dict(), 'epoch': args.epochs}, final_path)
    logger.info(f"Saved final model to {final_path}")
    print(f"\nTraining complete! Final model: {final_path}")
